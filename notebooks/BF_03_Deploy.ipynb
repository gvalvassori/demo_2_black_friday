{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "e0cef83a-3ede-403d-a616-7370f949a84f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFP SDK version: 2.5.0\n",
      "google-cloud-aiplatform==1.51.0\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component, Condition\n",
    "from kfp.registry import RegistryClient\n",
    "from typing import NamedTuple\n",
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "3115aece-31dc-4482-9502-3546a824469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"demoespecialidadgcp\"\n",
    "REGION = \"us-central1\"\n",
    "BUCKET_URI = f\"gs://demo-2-black-friday\"\n",
    "SERVICE_ACCOUNT = \"502688298240-compute@developer.gserviceaccount.com\"\n",
    "PIPELINE_ROOT = f\"{BUCKET_URI}/pipelines\"\n",
    "DATASET_ID = \"demo_2_black_friday\"\n",
    "TABLE_TRAIN = \"raw_train\"\n",
    "TABLE_TEST = \"raw_test\"\n",
    "\n",
    "! gcloud config set project {PROJECT_ID}\n",
    "BQ_REGION = REGION.split(\"-\")[0].upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a8357922-c5f1-4b5b-9b45-3a629ab0b27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d070f8fa-7c9a-4e66-8d07-9e5c396863d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"google-cloud-bigquery[pandas]==3.15.0\"],\n",
    "    base_image=\"python:3.10\"\n",
    ")\n",
    "def export_datasets(\n",
    "    project_id: str,\n",
    "    dataset_id: str,\n",
    "    table_train: str,\n",
    "    table_test: str,\n",
    "    dataset_train: Output[Dataset],\n",
    "    dataset_test: Output[Dataset]\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        project_id: The Project ID.\n",
    "        dataset_id: The BigQuery Dataset ID. Must be pre-created in the project.\n",
    "        table_train: The BigQuery train table name.\n",
    "        table_test: The BigQuery test table name.\n",
    "        \n",
    "    Returns:\n",
    "        dataset_train: The Dataset artifact with exported CSV file.\n",
    "        dataset_test: The Dataset artifact with exported CSV file.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    client = bigquery.Client(project=project_id)\n",
    "    table_name = f\"{project_id}.{dataset_id}.{table_train}\"\n",
    "    query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM {table_name}\n",
    "    \"\"\".format(\n",
    "        table_name=table_name\n",
    "    )\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query=query, job_config=job_config)    \n",
    "    df_train = query_job.result().to_dataframe()\n",
    "    \n",
    "    table_name = f\"{project_id}.{dataset_id}.{table_test}\"\n",
    "    query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM {table_name}\n",
    "    \"\"\".format(\n",
    "        table_name=table_name\n",
    "    )\n",
    "    job_config = bigquery.QueryJobConfig()\n",
    "    query_job = client.query(query=query, job_config=job_config)    \n",
    "    df_test = query_job.result().to_dataframe()\n",
    "    \n",
    "    df_train['source'] = 'train'\n",
    "    df_test['source'] = 'test'\n",
    "    \n",
    "    dataset = pd.concat([df_train, df_test])\n",
    "    \n",
    "    dataset['Age'] = dataset['Age'].apply(lambda x : str(x).replace('55+', '55'))\n",
    "    dataset['Stay_In_Current_City_Years'] = dataset['Stay_In_Current_City_Years'].apply(lambda x : str(x).replace('4+', '4'))\n",
    "    dataset.drop('Product_Category_3', axis = 1, inplace = True)\n",
    "    dataset.drop('User_ID', axis = 1, inplace = True)\n",
    "    dataset.drop('Product_ID', axis = 1, inplace = True)\n",
    "    dataset['Product_Category_2'].fillna(dataset['Product_Category_2'].median(), inplace = True)\n",
    "    dataset['Stay_In_Current_City_Years'] = dataset['Stay_In_Current_City_Years'].astype('int')\n",
    "    dataset.drop(['Gender', 'City_Category', 'Marital_Status'], axis = 1, inplace = True)\n",
    "    \n",
    "    train = dataset.loc[dataset['source'] == 'train']\n",
    "    test = dataset.loc[dataset['source'] == 'test']\n",
    "    train.drop('source', axis = 1, inplace = True)\n",
    "    test.drop('source', axis = 1, inplace = True)\n",
    "    \n",
    "    train.to_csv(dataset_train.path + '.csv', index=False)\n",
    "    test.to_csv(dataset_test.path + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0156d739-3fd2-4e17-aa31-94389bd3ee1d",
   "metadata": {},
   "source": [
    "# Model Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "48da8720-22bd-4b5c-95b7-f4d4548267bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",        \n",
    "        \"scikit-learn==1.4.2\",\n",
    "        \"scipy==1.13.0\",\n",
    "        \"xgboost==2.0.3\"\n",
    "    ],\n",
    "    base_image=\"python:3.10\"\n",
    ")\n",
    "def train_model(\n",
    "    dataset_train: Input[Dataset],\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics],\n",
    "):\n",
    "    \"\"\"Training XGBoost Regressor model for demo-2-black-friday.\n",
    "\n",
    "    Args:\n",
    "        dataset_train: The training dataset.\n",
    "\n",
    "    Returns:\n",
    "        model: The model artifact stores the model.joblib file.\n",
    "        metrics: The metrics of the trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import time, os, joblib\n",
    "    from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    from sklearn.preprocessing import OrdinalEncoder, StandardScaler\n",
    "    from sklearn.compose import ColumnTransformer\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from xgboost import XGBRegressor\n",
    "    \n",
    "    with open(dataset_train.path + '.csv', \"r\") as train_data:\n",
    "        dataset = pd.read_csv(train_data)\n",
    "        \n",
    "    X = dataset.drop(\"Purchase\", axis = 1)\n",
    "    Y = dataset[\"Purchase\"]\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 42)\n",
    "    \n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "        ('cat', OrdinalEncoder(), ['Age'])],\n",
    "    )\n",
    "    \n",
    "    max_depth = [int(x) for x in np.linspace(start = 5, stop = 20, num = 15)]\n",
    "    learning_rate = ['0.01', '0.05', '0.1', '0.25', '0.5', '0.75', '1.0']\n",
    "    min_child_weight = [int(x) for x in np.linspace(start = 45, stop = 70, num = 15)]\n",
    "    \n",
    "    params = {\n",
    "     \"regressor__learning_rate\"    : learning_rate,\n",
    "     \"regressor__max_depth\"        : max_depth,\n",
    "     \"regressor__min_child_weight\" : min_child_weight,\n",
    "     \"regressor__gamma\"            : [0.0, 0.1, 0.2 , 0.3, 0.4],\n",
    "     \"regressor__colsample_bytree\" : [0.3, 0.4, 0.5 , 0.7]\n",
    "    }\n",
    "    \n",
    "    xgb = XGBRegressor(verbosity = 0, random_state = 42)\n",
    "       \n",
    "    regr = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('standard-scaler', StandardScaler()),\n",
    "        ('regressor', xgb)\n",
    "    ])\n",
    "    \n",
    "    xgb_cv = RandomizedSearchCV(regr, param_distributions = params, cv = 5, random_state = 42)\n",
    "    \n",
    "    xgb_cv.fit(X_train, Y_train)\n",
    "      \n",
    "    xgb_best = xgb_cv.best_estimator_\n",
    "    \n",
    "    Y_pred_xgb_best = xgb_best.predict(X_test)\n",
    "    \n",
    "    r2 = r2_score(Y_test, Y_pred_xgb_best)\n",
    "    rmse = np.sqrt(mean_squared_error(Y_test, Y_pred_xgb_best))\n",
    "    \n",
    "    metrics.log_metric(\"Framework\", \"XGBoost\")\n",
    "    metrics.log_metric(\"Train_samples_size\", len(X_train))\n",
    "    metrics.log_metric(\"Validation_samples_size\", len(X_test))\n",
    "    metrics.log_metric(\"RMSE\", round(rmse,2))\n",
    "    metrics.log_metric(\"R2 score\", round(r2,2))\n",
    "    \n",
    "    print(\"XGB regression:\")\n",
    "    print(\"RMSE:\",rmse)\n",
    "    print(\"R2 score:\", r2)\n",
    "    \n",
    "    # Export the model to a file\n",
    "    os.makedirs(model.path, exist_ok=True)\n",
    "    joblib.dump(xgb_best, os.path.join(model.path, \"model.joblib\"))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4812066-6e07-4c58-aedb-98a8c686f502",
   "metadata": {},
   "source": [
    "# Model Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80082310-39c6-4a9c-b7de-88c36212f40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bc5a0adf-2dcd-4c47-be7b-11037e938142",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",        \n",
    "        \"scikit-learn==1.4.2\",\n",
    "        \"joblib==1.2.0\",\n",
    "        \"xgboost==2.0.3\"\n",
    "    ],\n",
    "    base_image=\"python:3.10\"\n",
    ")\n",
    "def evaluate_model(\n",
    "    dataset_test: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    metrics: Output[Metrics]\n",
    ") -> NamedTuple('EvaluationOutput', [('r2', float), ('rmse', float)]):\n",
    "    \"\"\"Evaluate the trained model with test data.\n",
    "\n",
    "    Args:\n",
    "        dataset_test: The testing dataset.\n",
    "        model: The trained model.\n",
    "        \n",
    "    Returns:\n",
    "        metrics: The evaluation metrics of the model.\n",
    "        r2: The R2 score of the model.\n",
    "        rmse: The RMSE of the model.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import joblib\n",
    "    from sklearn.metrics import r2_score, mean_squared_error\n",
    "    from xgboost import XGBRegressor\n",
    "    try:\n",
    "        # Load the test dataset\n",
    "        with open(dataset_test.path + '.csv', \"r\") as test_data:\n",
    "            test_dataset = pd.read_csv(test_data)\n",
    "\n",
    "        X_test = test_dataset.drop(\"Purchase\", axis=1)\n",
    "        Y_test = test_dataset[\"Purchase\"]\n",
    "\n",
    "        # Load the trained model\n",
    "        model_file = model.path + \"/model.joblib\"\n",
    "        trained_model = joblib.load(model_file)\n",
    "\n",
    "        # Predict with the model\n",
    "        Y_pred = trained_model.predict(X_test)\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        r2 = r2_score(Y_test, Y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
    "\n",
    "        metrics.log_metric(\"RMSE\", round(rmse, 2))\n",
    "        metrics.log_metric(\"R2 score\", round(r2, 2))\n",
    "\n",
    "        print(\"Evaluation results:\")\n",
    "        print(\"RMSE:\", rmse)\n",
    "        print(\"R2 score:\", r2)\n",
    "\n",
    "        from collections import namedtuple\n",
    "        EvaluationOutput = namedtuple('EvaluationOutput', ['r2', 'rmse'])\n",
    "        return EvaluationOutput(r2=float(r2), rmse=float(rmse))\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model evaluation: {str(e)}\")\n",
    "        raise\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d64d1e8c-63aa-4643-8515-ed5f3d407a46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\n",
    "        \"pandas==2.2.2\",        \n",
    "        \"scikit-learn==1.4.2\",\n",
    "        \"xgboost==2.0.3\"\n",
    "    ],\n",
    "    base_image=\"python:3.10\"\n",
    ")\n",
    "def inference(\n",
    "    dataset_test: Input[Dataset],\n",
    "    model: Input[Model],\n",
    "    predictions: Output[Dataset]\n",
    "):\n",
    "    \"\"\"Perform inference with the trained model on the test dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_test: The testing dataset.\n",
    "        model: The trained model.\n",
    "        \n",
    "    Returns:\n",
    "        predictions: The dataset with predictions.\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    # Load the test dataset\n",
    "    with open(dataset_test.path + '.csv', \"r\") as test_data:\n",
    "        test_dataset = pd.read_csv(test_data)\n",
    "        \n",
    "    X_test = test_dataset.drop(\"Purchase\", axis=1)\n",
    "    \n",
    "    # Load the trained model\n",
    "    model_file = model.path + \"/model.joblib\"\n",
    "    trained_model = joblib.load(model_file)\n",
    "    \n",
    "    # Predict with the model\n",
    "    Y_pred = trained_model.predict(X_test)\n",
    "    \n",
    "    # Save predictions to the output\n",
    "    predictions_df = test_dataset.copy()\n",
    "    predictions_df[\"Predicted_Purchase\"] = Y_pred\n",
    "    predictions_df.to_csv(predictions.path + '.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e984963b-ac8c-469d-a9f8-6553855ba334",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_20098/3778412692.py:26: DeprecationWarning: dsl.Condition is deprecated. Please use dsl.If instead.\n",
      "  with dsl.Condition(evaluate_model_task.outputs['r2'] < 0.5):\n"
     ]
    }
   ],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"demo_2_black_friday\",\n",
    ")\n",
    "def pipeline(\n",
    "    PROJECT_ID: str,\n",
    "    DATASET_ID: str,\n",
    "    TABLE_TRAIN: str,\n",
    "    TABLE_TEST: str,\n",
    "):\n",
    "    export_dataset_task = export_datasets(\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        table_train=TABLE_TRAIN,\n",
    "        table_test=TABLE_TEST,\n",
    "    )\n",
    "    \n",
    "    train_model_task = train_model(\n",
    "        dataset_train=export_dataset_task.outputs[\"dataset_train\"],\n",
    "    )\n",
    "    \n",
    "    evaluate_model_task = evaluate_model(\n",
    "        dataset_test=export_dataset_task.outputs[\"dataset_train\"],\n",
    "        model=train_model_task.outputs[\"model\"],\n",
    "    )\n",
    "    \n",
    "    with dsl.Condition(evaluate_model_task.outputs['r2'] < 0.5):\n",
    "        inference_task = inference(\n",
    "            dataset_test=export_dataset_task.outputs[\"dataset_test\"],\n",
    "            model=train_model_task.outputs[\"model\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f5574646-d821-4e74-9cad-0edfc3da45f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipe_comps/demo_2_black_friday_pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "667b151e-d053-4a48-8c24-99c51c280096",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"demo_2_black_friday\",\n",
    ")\n",
    "def pipeline(\n",
    "    PROJECT_ID: str,\n",
    "    DATASET_ID: str,\n",
    "    TABLE_TRAIN: str,\n",
    "    TABLE_TEST: str,\n",
    "    ):\n",
    "    export_dataset_task = export_datasets(\n",
    "        project_id=PROJECT_ID,\n",
    "        dataset_id=DATASET_ID,\n",
    "        table_train=TABLE_TRAIN,\n",
    "        table_test=TABLE_TEST,\n",
    "    )\n",
    "    \n",
    "    train_model_task = train_model(\n",
    "        dataset_train=export_dataset_task.outputs[\"dataset_train\"],\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ba2fadd9-25a5-426c-b052-294a455fffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "compiler.Compiler().compile(pipeline_func=pipeline, package_path=\"pipe_comps/demo_2_black_friday_pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d3a24b1-f583-4f64-a703-39ac52df5206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/502688298240/locations/us-central1/pipelineJobs/demo-2-black-friday-20240717190023\n",
      "To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/502688298240/locations/us-central1/pipelineJobs/demo-2-black-friday-20240717190023')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/demo-2-black-friday-20240717190023?project=502688298240\n",
      "PipelineJob projects/502688298240/locations/us-central1/pipelineJobs/demo-2-black-friday-20240717190023 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/502688298240/locations/us-central1/pipelineJobs/demo-2-black-friday-20240717190023 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/502688298240/locations/us-central1/pipelineJobs/demo-2-black-friday-20240717190023 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob projects/502688298240/locations/us-central1/pipelineJobs/demo-2-black-friday-20240717190023 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "PipelineJob run completed. Resource name: projects/502688298240/locations/us-central1/pipelineJobs/demo-2-black-friday-20240717190023\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"demo_2_black_friday\",\n",
    "    template_path=\"pipe_comps/demo_2_black_friday_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    "    parameter_values={\n",
    "        'PROJECT_ID': PROJECT_ID,\n",
    "        'DATASET_ID': DATASET_ID,\n",
    "        'TABLE_TRAIN': TABLE_TRAIN,\n",
    "        'TABLE_TEST': TABLE_TEST\n",
    "    },\n",
    "    enable_caching=True,\n",
    ")\n",
    "\n",
    "job.run(service_account=SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53600a4e-1f1c-4a75-a5b1-036965057a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "client_registry = RegistryClient(host=f\"https://us-central1-kfp.pkg.dev/demoespecialidadgcp/demo-2-black-friday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "cc697a1b-ed58-4d09-9c91-4d54d98ceb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "templateName, versionName = client_registry.upload_pipeline(\n",
    "  file_name=\"pipe_comps/demo_2_black_friday_pipeline.yaml\",\n",
    "  tags=[\"latest\"],\n",
    "  extra_headers={\"description\":\"Pipeline para la transformacion de datos, entrenamiento de modelos, registro de metricas, predicciones y modelo, y despliegue del modelo\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ea7da-832f-467b-8123-f6107eeb09c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m121",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m121"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
